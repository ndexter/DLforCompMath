{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWW1TyjaecRh"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOtR1FzCef-u"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zr7KpBhMcYvE"
   },
   "source": [
    "# Build a linear model with Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJl4gaPFzxQz"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/estimators/linear\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/estimators/linear.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimators/linear.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77aETSYDcdoK"
   },
   "source": [
    "This tutorial uses the `tf.estimator` API in TensorFlow to solve a benchmark binary classification problem. Estimators are TensorFlow's most scalable and production-oriented model type. For more information see the [Estimator guide](https://www.tensorflow.org/guide/estimators).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Using census data which contains data about a person's age, education, marital status, and occupation (the *features*), we will try to predict whether or not the person earns more than 50,000 dollars a year (the target *label*). We will train a *logistic regression* model that, given an individual's information, outputs a number between 0 and 1—this can be interpreted as the probability that the individual has an annual income of over 50,000 dollars.\n",
    "\n",
    "Key Point: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is each  feature relevant to the problem you want to solve or will it introduce bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/).\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import TensorFlow, feature column support, and supporting modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQgONe5ecYvE"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.feature_column as fc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rpb1JSMj1nqk"
   },
   "source": [
    "And let's enable [eager execution](https://www.tensorflow.org/guide/eager) to inspect this program as we run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQzxON782Eby"
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MPr95UccYvL"
   },
   "source": [
    "## Download the official implementation\n",
    "\n",
    "We'll use the [wide and deep model](https://github.com/tensorflow/models/tree/master/official/wide_deep/) available in TensorFlow's [model repository](https://github.com/tensorflow/models/). Download the code, add the root directory to your Python path, and jump to the `wide_deep` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTwQzWcn8aBu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.24.3)\n",
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 3217, done.\u001b[K\n",
      "remote: Counting objects: 100% (3217/3217), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2742/2742), done.\u001b[K\n",
      "^Cceiving objects:  61% (1990/3217), 201.14 MiB | 15.09 MiB/s   \n"
     ]
    }
   ],
   "source": [
    "! pip install requests\n",
    "! git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRpuysc73Eb-"
   },
   "source": [
    "Add the root directory of the repository to your Python path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVvFyhnkcYvL"
   },
   "outputs": [],
   "source": [
    "models_path = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "sys.path.append(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15Ethw-wcYvP"
   },
   "source": [
    "Download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QilS4-0cYvQ"
   },
   "outputs": [],
   "source": [
    "from official.wide_deep import census_dataset\n",
    "from official.wide_deep import census_main\n",
    "\n",
    "census_dataset.download(\"/tmp/census_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cD5e3ibAcYvS"
   },
   "source": [
    "### Command line usage\n",
    "\n",
    "The repo includes a complete program for experimenting with this type of model.\n",
    "\n",
    "To execute the tutorial code from the command line first add the path to tensorflow/models to your `PYTHONPATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYOkY8boUptJ"
   },
   "outputs": [],
   "source": [
    "#export PYTHONPATH=${PYTHONPATH}:\"$(pwd)/models\"\n",
    "#running from python you need to set the `os.environ` or the subprocess will not see the directory.\n",
    "\n",
    "if \"PYTHONPATH\" in os.environ:\n",
    "  os.environ['PYTHONPATH'] += os.pathsep +  models_path\n",
    "else:\n",
    "  os.environ['PYTHONPATH'] = models_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5r0V9YUMUyoh"
   },
   "source": [
    "Use `--help` to see what command line options are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_3tBaLW4YM4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DNN on census income dataset.\r\n",
      "flags:\r\n",
      "\r\n",
      "/home/jupyter/DLforCompMath/Tutorial2/models/official/wide_deep/census_main.py:\r\n",
      "  -bs,--batch_size:\r\n",
      "    Batch size for training and evaluation. When using multiple gpus, this is\r\n",
      "    the\r\n",
      "    global batch size for all devices. For example, if the batch size is 32 and\r\n",
      "    there are 4 GPUs, each GPU will get 8 examples on each step.\r\n",
      "    (default: '40')\r\n",
      "    (an integer)\r\n",
      "  --[no]clean:\r\n",
      "    If set, model_dir will be removed if it exists.\r\n",
      "    (default: 'false')\r\n",
      "  -dd,--data_dir:\r\n",
      "    The location of the input data.\r\n",
      "    (default: '/tmp/census_data')\r\n",
      "  --[no]download_if_missing:\r\n",
      "    Download data to data_dir if it is not already present.\r\n",
      "    (default: 'true')\r\n",
      "  -ebe,--epochs_between_evals:\r\n",
      "    The number of training epochs to run between evaluations.\r\n",
      "    (default: '2')\r\n",
      "    (an integer)\r\n",
      "  -ed,--export_dir:\r\n",
      "    If set, a SavedModel serialization of the model will be exported to this\r\n",
      "    directory at the end of training. See the README for more details and\r\n",
      "    relevant\r\n",
      "    links.\r\n",
      "  -hk,--hooks:\r\n",
      "    A list of (case insensitive) strings to specify the names of training hooks.\r\n",
      "    ﻿  Hook:\r\n",
      "    ﻿    loggingtensorhook\r\n",
      "    ﻿    profilerhook\r\n",
      "    ﻿    examplespersecondhook\r\n",
      "    ﻿    loggingmetrichook\r\n",
      "    ﻿    stepcounterhook\r\n",
      "    ﻿  Example: `--hooks ProfilerHook,ExamplesPerSecondHook`\r\n",
      "    See official.utils.logs.hooks_helper for details.\r\n",
      "    (default: 'LoggingTensorHook')\r\n",
      "    (a comma separated list)\r\n",
      "  -md,--model_dir:\r\n",
      "    The location of the model checkpoint files.\r\n",
      "    (default: '/tmp/census_model')\r\n",
      "  -mt,--model_type: <wide|deep|wide_deep>: Select model topology.\r\n",
      "    (default: 'wide_deep')\r\n",
      "  -te,--train_epochs:\r\n",
      "    The number of epochs used to train.\r\n",
      "    (default: '40')\r\n",
      "    (an integer)\r\n",
      "\r\n",
      "Try --helpfull to get a list of all flags.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m official.wide_deep.census_main --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrMLazEN6DMj"
   },
   "source": [
    "Now run the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "py7MarZl5Yh6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0723 06:40:58.792081 139835868321600 estimator.py:201] Using config: {'_model_dir': '/tmp/census_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 0\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2dec3846d8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W0723 06:40:58.792726 139835868321600 tf_logging.py:161] 'cpuinfo' not imported. CPU info will not be logged.\n",
      "I0723 06:40:59.021049 139835868321600 logger.py:152] Benchmark run: {'model_name': 'wide_deep', 'dataset': {'name': 'Census Income'}, 'machine_config': {'memory_total': 63332204544, 'memory_available': 55558737920}, 'test_id': None, 'run_date': '2019-07-23T06:40:58.792432Z', 'tensorflow_version': {'version': '1.13.1', 'git_hash': \"b'v1.13.0-rc2-5-g6612da8'\"}, 'tensorflow_environment_variables': [], 'run_parameters': [{'name': 'batch_size', 'long_value': 40}, {'name': 'model_type', 'string_value': 'wide'}, {'name': 'train_epochs', 'long_value': 2}]}\n",
      "W0723 06:40:59.035968 139835868321600 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "I0723 06:40:59.049594 139835868321600 census_dataset.py:167] Parsing /tmp/census_data/adult.data\n",
      "I0723 06:40:59.082082 139835868321600 estimator.py:1111] Calling model_fn.\n",
      "W0723 06:40:59.125016 139835868321600 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "W0723 06:40:59.140262 139835868321600 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2898: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "I0723 06:41:00.158206 139835868321600 estimator.py:1113] Done calling model_fn.\n",
      "I0723 06:41:00.158526 139835868321600 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0723 06:41:00.500226 139835868321600 monitored_session.py:222] Graph was finalized.\n",
      "2019-07-23 06:41:00.500532: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-07-23 06:41:00.506329: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2399995000 Hz\n",
      "2019-07-23 06:41:00.507337: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5603d2058bd0 executing computations on platform Host. Devices:\n",
      "2019-07-23 06:41:00.507370: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I0723 06:41:00.608386 139835868321600 session_manager.py:491] Running local_init_op.\n",
      "I0723 06:41:00.628953 139835868321600 session_manager.py:493] Done running local_init_op.\n",
      "I0723 06:41:01.255845 139835868321600 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/census_model/model.ckpt.\n",
      "I0723 06:41:01.805459 139835868321600 basic_session_run_hooks.py:249] average_loss = 0.6931472, loss = 27.725887\n",
      "I0723 06:41:01.805792 139835868321600 basic_session_run_hooks.py:249] loss = 27.725887, step = 1\n",
      "I0723 06:41:02.631333 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 121.043\n",
      "I0723 06:41:02.632013 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.3199336, loss = 12.797344 (0.827 sec)\n",
      "I0723 06:41:02.632221 139835868321600 basic_session_run_hooks.py:247] loss = 12.797344, step = 101 (0.826 sec)\n",
      "I0723 06:41:03.239322 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 164.475\n",
      "I0723 06:41:03.239976 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.54833627, loss = 21.93345 (0.608 sec)\n",
      "I0723 06:41:03.240240 139835868321600 basic_session_run_hooks.py:247] loss = 21.93345, step = 201 (0.608 sec)\n",
      "I0723 06:41:03.909388 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 149.231\n",
      "I0723 06:41:03.909821 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.29845235, loss = 11.938094 (0.670 sec)\n",
      "I0723 06:41:03.909979 139835868321600 basic_session_run_hooks.py:247] loss = 11.938094, step = 301 (0.670 sec)\n",
      "I0723 06:41:04.508266 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 166.976\n",
      "I0723 06:41:04.508673 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.33492, loss = 13.396799 (0.599 sec)\n",
      "I0723 06:41:04.508881 139835868321600 basic_session_run_hooks.py:247] loss = 13.396799, step = 401 (0.599 sec)\n",
      "I0723 06:41:05.106138 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 167.264\n",
      "I0723 06:41:05.106566 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.34177732, loss = 13.671093 (0.598 sec)\n",
      "I0723 06:41:05.106713 139835868321600 basic_session_run_hooks.py:247] loss = 13.671093, step = 501 (0.598 sec)\n",
      "I0723 06:41:05.637601 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 188.156\n",
      "I0723 06:41:05.638071 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.46984863, loss = 18.793945 (0.532 sec)\n",
      "I0723 06:41:05.638243 139835868321600 basic_session_run_hooks.py:247] loss = 18.793945, step = 601 (0.532 sec)\n",
      "I0723 06:41:06.231111 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 168.491\n",
      "I0723 06:41:06.231548 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.43672633, loss = 17.469053 (0.593 sec)\n",
      "I0723 06:41:06.231697 139835868321600 basic_session_run_hooks.py:247] loss = 17.469053, step = 701 (0.593 sec)\n",
      "I0723 06:41:06.905609 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 148.256\n",
      "I0723 06:41:06.906076 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.3144568, loss = 12.578272 (0.675 sec)\n",
      "I0723 06:41:06.906367 139835868321600 basic_session_run_hooks.py:247] loss = 12.578272, step = 801 (0.675 sec)\n",
      "I0723 06:41:07.522839 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 162.022\n",
      "I0723 06:41:07.523296 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.24652, loss = 9.8608 (0.617 sec)\n",
      "I0723 06:41:07.523501 139835868321600 basic_session_run_hooks.py:247] loss = 9.8608, step = 901 (0.617 sec)\n",
      "I0723 06:41:08.116213 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 168.523\n",
      "I0723 06:41:08.116696 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.23987398, loss = 9.594959 (0.593 sec)\n",
      "I0723 06:41:08.116926 139835868321600 basic_session_run_hooks.py:247] loss = 9.594959, step = 1001 (0.593 sec)\n",
      "I0723 06:41:08.716981 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 166.451\n",
      "I0723 06:41:08.717390 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.32646936, loss = 13.058774 (0.601 sec)\n",
      "I0723 06:41:08.717560 139835868321600 basic_session_run_hooks.py:247] loss = 13.058774, step = 1101 (0.601 sec)\n",
      "I0723 06:41:09.316932 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 166.679\n",
      "I0723 06:41:09.317344 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.29213083, loss = 11.685233 (0.600 sec)\n",
      "I0723 06:41:09.317477 139835868321600 basic_session_run_hooks.py:247] loss = 11.685233, step = 1201 (0.600 sec)\n",
      "I0723 06:41:09.924275 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 164.651\n",
      "I0723 06:41:09.924882 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.2975624, loss = 11.902495 (0.608 sec)\n",
      "I0723 06:41:09.925041 139835868321600 basic_session_run_hooks.py:247] loss = 11.902495, step = 1301 (0.608 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0723 06:41:10.532779 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 164.341\n",
      "I0723 06:41:10.533220 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.3098352, loss = 12.393408 (0.608 sec)\n",
      "I0723 06:41:10.533364 139835868321600 basic_session_run_hooks.py:247] loss = 12.393408, step = 1401 (0.608 sec)\n",
      "I0723 06:41:11.128962 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 167.737\n",
      "I0723 06:41:11.129641 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.3877038, loss = 15.508152 (0.596 sec)\n",
      "I0723 06:41:11.129849 139835868321600 basic_session_run_hooks.py:247] loss = 15.508152, step = 1501 (0.596 sec)\n",
      "I0723 06:41:11.726249 139835868321600 basic_session_run_hooks.py:680] global_step/sec: 167.421\n",
      "I0723 06:41:11.726667 139835868321600 basic_session_run_hooks.py:247] average_loss = 0.20675388, loss = 8.270155 (0.597 sec)\n",
      "I0723 06:41:11.726832 139835868321600 basic_session_run_hooks.py:247] loss = 8.270155, step = 1601 (0.597 sec)\n",
      "I0723 06:41:11.919805 139835868321600 basic_session_run_hooks.py:594] Saving checkpoints for 1629 into /tmp/census_model/model.ckpt.\n",
      "I0723 06:41:12.044300 139835868321600 estimator.py:359] Loss for final step: 0.25357318.\n",
      "I0723 06:41:12.053295 139835868321600 census_dataset.py:167] Parsing /tmp/census_data/adult.test\n",
      "I0723 06:41:12.076283 139835868321600 estimator.py:1111] Calling model_fn.\n",
      "W0723 06:41:12.954774 139835868321600 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0723 06:41:13.351664 139835868321600 metrics_impl.py:783] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0723 06:41:13.455393 139835868321600 metrics_impl.py:783] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I0723 06:41:13.474642 139835868321600 estimator.py:1113] Done calling model_fn.\n",
      "I0723 06:41:13.492748 139835868321600 evaluation.py:257] Starting evaluation at 2019-07-23T06:41:13Z\n",
      "I0723 06:41:13.626562 139835868321600 monitored_session.py:222] Graph was finalized.\n",
      "W0723 06:41:13.626953 139835868321600 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0723 06:41:13.629437 139835868321600 saver.py:1270] Restoring parameters from /tmp/census_model/model.ckpt-1629\n",
      "I0723 06:41:13.716293 139835868321600 session_manager.py:491] Running local_init_op.\n",
      "I0723 06:41:13.754061 139835868321600 session_manager.py:493] Done running local_init_op.\n",
      "I0723 06:41:16.872105 139835868321600 evaluation.py:277] Finished evaluation at 2019-07-23-06:41:16\n",
      "I0723 06:41:16.872342 139835868321600 estimator.py:1979] Saving dict for global step 1629: accuracy = 0.83649653, accuracy_baseline = 0.76377374, auc = 0.88440543, auc_precision_recall = 0.696075, average_loss = 0.3509894, global_step = 1629, label/mean = 0.23622628, loss = 14.006025, precision = 0.6832817, prediction/mean = 0.24710622, recall = 0.57384294\n",
      "I0723 06:41:17.079600 139835868321600 estimator.py:2039] Saving 'checkpoint_path' summary for global step 1629: /tmp/census_model/model.ckpt-1629\n",
      "I0723 06:41:17.080217 139835868321600 wide_deep_run_loop.py:116] Results at epoch 2 / 2\n",
      "I0723 06:41:17.080307 139835868321600 wide_deep_run_loop.py:117] ------------------------------------------------------------\n",
      "I0723 06:41:17.080380 139835868321600 wide_deep_run_loop.py:120] accuracy: 0.83649653\n",
      "I0723 06:41:17.080439 139835868321600 wide_deep_run_loop.py:120] accuracy_baseline: 0.76377374\n",
      "I0723 06:41:17.080504 139835868321600 wide_deep_run_loop.py:120] auc: 0.88440543\n",
      "I0723 06:41:17.080568 139835868321600 wide_deep_run_loop.py:120] auc_precision_recall: 0.696075\n",
      "I0723 06:41:17.080630 139835868321600 wide_deep_run_loop.py:120] average_loss: 0.3509894\n",
      "I0723 06:41:17.080679 139835868321600 wide_deep_run_loop.py:120] global_step: 1629\n",
      "I0723 06:41:17.080725 139835868321600 wide_deep_run_loop.py:120] label/mean: 0.23622628\n",
      "I0723 06:41:17.080784 139835868321600 wide_deep_run_loop.py:120] loss: 14.006025\n",
      "I0723 06:41:17.080831 139835868321600 wide_deep_run_loop.py:120] precision: 0.6832817\n",
      "I0723 06:41:17.080872 139835868321600 wide_deep_run_loop.py:120] prediction/mean: 0.24710622\n",
      "I0723 06:41:17.080913 139835868321600 wide_deep_run_loop.py:120] recall: 0.57384294\n",
      "I0723 06:41:17.081013 139835868321600 logger.py:147] Benchmark metric: {'name': 'accuracy', 'value': 0.8364965319633484, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.080977Z', 'extras': []}\n",
      "I0723 06:41:17.081110 139835868321600 logger.py:147] Benchmark metric: {'name': 'accuracy_baseline', 'value': 0.7637737393379211, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081092Z', 'extras': []}\n",
      "I0723 06:41:17.081222 139835868321600 logger.py:147] Benchmark metric: {'name': 'auc', 'value': 0.8844054341316223, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081203Z', 'extras': []}\n",
      "I0723 06:41:17.081308 139835868321600 logger.py:147] Benchmark metric: {'name': 'auc_precision_recall', 'value': 0.6960750222206116, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081292Z', 'extras': []}\n",
      "I0723 06:41:17.081388 139835868321600 logger.py:147] Benchmark metric: {'name': 'average_loss', 'value': 0.3509894013404846, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081373Z', 'extras': []}\n",
      "I0723 06:41:17.081469 139835868321600 logger.py:147] Benchmark metric: {'name': 'label/mean', 'value': 0.23622627556324005, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081453Z', 'extras': []}\n",
      "I0723 06:41:17.081577 139835868321600 logger.py:147] Benchmark metric: {'name': 'loss', 'value': 14.006025314331055, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081563Z', 'extras': []}\n",
      "I0723 06:41:17.081667 139835868321600 logger.py:147] Benchmark metric: {'name': 'precision', 'value': 0.6832817196846008, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081653Z', 'extras': []}\n",
      "I0723 06:41:17.081738 139835868321600 logger.py:147] Benchmark metric: {'name': 'prediction/mean', 'value': 0.24710622429847717, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081725Z', 'extras': []}\n",
      "I0723 06:41:17.081841 139835868321600 logger.py:147] Benchmark metric: {'name': 'recall', 'value': 0.5738429427146912, 'unit': None, 'global_step': 1629, 'timestamp': '2019-07-23T06:41:17.081826Z', 'extras': []}\n"
     ]
    }
   ],
   "source": [
    "!python -m official.wide_deep.census_main --model_type=wide --train_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmZ4CpaOcYvV"
   },
   "source": [
    "## Read the U.S. Census data\n",
    "\n",
    "This example uses the [U.S Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income) from 1994 and 1995. We have provided the [census_dataset.py](https://github.com/tensorflow/models/tree/master/official/wide_deep/census_dataset.py) script to download the data and perform a little cleanup.\n",
    "\n",
    "Since the task is a *binary classification problem*, we'll construct a label column named \"label\" whose value is 1 if the income is over 50K, and 0 otherwise. For reference, see the `input_fn` in [census_main.py](https://github.com/tensorflow/models/tree/master/official/wide_deep/census_main.py).\n",
    "\n",
    "Let's look at the data to see which columns we can use to predict the target label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6Tgye8bcYvX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult.data  adult.test\r\n"
     ]
    }
   ],
   "source": [
    "!ls  /tmp/census_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6y3mj9zKcYva"
   },
   "outputs": [],
   "source": [
    "train_file = \"/tmp/census_data/adult.data\"\n",
    "test_file = \"/tmp/census_data/adult.test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EO_McKgE5il2"
   },
   "source": [
    "[pandas](https://pandas.pydata.org/) provides some convenient utilities for data analysis. Here's a list of columns available in the Census Income dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkn1FNmpcYvb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital_status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \n",
       "0          2174             0              40  United-States          <=50K  \n",
       "1             0             0              13  United-States          <=50K  \n",
       "2             0             0              40  United-States          <=50K  \n",
       "3             0             0              40  United-States          <=50K  \n",
       "4             0             0              40           Cuba          <=50K  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "train_df = pandas.read_csv(train_file, header = None, names = census_dataset._CSV_COLUMNS)\n",
    "test_df = pandas.read_csv(test_file, header = None, names = census_dataset._CSV_COLUMNS)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZZtXes4cYvf"
   },
   "source": [
    "The columns are grouped into two types: *categorical* and *continuous* columns:\n",
    "\n",
    "* A column is called *categorical* if its value can only be one of the categories in a finite set. For example, the relationship status of a person (wife, husband, unmarried, etc.) or the education level (high school, college, etc.) are categorical columns.\n",
    "* A column is called *continuous* if its value can be any numerical value in a continuous range. For example, the capital gain of a person (e.g. $14,084) is a continuous column.\n",
    "\n",
    "## Converting Data into Tensors\n",
    "\n",
    "When building a `tf.estimator` model, the input data is specified by using an *input function* (or `input_fn`). This builder function returns a `tf.data.Dataset` of batches of `(features-dict, label)` pairs. It is not called until it is passed to `tf.estimator.Estimator` methods such as `train` and `evaluate`.\n",
    "\n",
    "The input builder function returns the following pair:\n",
    "\n",
    "1. `features`: A dict from feature names to `Tensors` or `SparseTensors` containing batches of features.\n",
    "2. `labels`: A `Tensor` containing batches of labels.\n",
    "\n",
    "The keys of the `features` are used to configure the model's input layer.\n",
    "\n",
    "Note: The input function is called while constructing the TensorFlow graph, *not* while running the graph. It is returning a representation of the input data as a sequence of TensorFlow graph operations.\n",
    "\n",
    "For small problems like this, it's easy to make a `tf.data.Dataset` by slicing the `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7zNJflKcYvg"
   },
   "outputs": [],
   "source": [
    "def easy_input_function(df, label_key, num_epochs, shuffle, batch_size):\n",
    "  label = df[label_key]\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df),label))\n",
    "\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(10000)\n",
    "\n",
    "  ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeEgNR9AcYvh"
   },
   "source": [
    "Since we have eager execution enabled, it's easy to inspect the resulting dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygaKuikecYvi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 06:41:43.805676 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature keys: ['age', 'workclass', 'fnlwgt', 'education', 'education_num']\n",
      "\n",
      "A batch of Ages  : tf.Tensor([27 66 21 64 29 39 41 48 22 39], shape=(10,), dtype=int32)\n",
      "\n",
      "A batch of Labels: tf.Tensor(\n",
      "[b'<=50K' b'<=50K' b'<=50K' b'<=50K' b'<=50K' b'<=50K' b'>50K' b'>50K'\n",
      " b'<=50K' b'<=50K'], shape=(10,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ds = easy_input_function(train_df, label_key='income_bracket', num_epochs=5, shuffle=True, batch_size=10)\n",
    "\n",
    "for feature_batch, label_batch in ds.take(1):\n",
    "  print('Some feature keys:', list(feature_batch.keys())[:5])\n",
    "  print()\n",
    "  print('A batch of Ages  :', feature_batch['age'])\n",
    "  print()\n",
    "  print('A batch of Labels:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_KZxQUucYvm"
   },
   "source": [
    "But this approach has severly-limited scalability. Larger datasets should be streamed from disk. The `census_dataset.input_fn` provides an example of how to do this using `tf.decode_csv` and `tf.data.TextLineDataset`:\n",
    "\n",
    "<!-- TODO(markdaoust): This `input_fn` should use `tf.contrib.data.make_csv_dataset` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUTeXaEUcYvn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
      "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
      "  assert tf.gfile.Exists(data_file), (\n",
      "      '%s not found. Please make sure you have run census_dataset.py and '\n",
      "      'set the --data_dir argument to the correct path.' % data_file)\n",
      "\n",
      "  def parse_csv(value):\n",
      "    tf.logging.info('Parsing {}'.format(data_file))\n",
      "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
      "    features = dict(zip(_CSV_COLUMNS, columns))\n",
      "    labels = features.pop('income_bracket')\n",
      "    classes = tf.equal(labels, '>50K')  # binary classification\n",
      "    return features, classes\n",
      "\n",
      "  # Extract lines from input files using the Dataset API.\n",
      "  dataset = tf.data.TextLineDataset(data_file)\n",
      "\n",
      "  if shuffle:\n",
      "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
      "\n",
      "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
      "\n",
      "  # We call repeat after shuffling, rather than before, to prevent separate\n",
      "  # epochs from blending together.\n",
      "  dataset = dataset.repeat(num_epochs)\n",
      "  dataset = dataset.batch(batch_size)\n",
      "  return dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(census_dataset.input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyGcv_e-cYvq"
   },
   "source": [
    "This `input_fn` returns equivalent output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mv3as_CEcYvu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Parsing /tmp/census_data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:41:51.094161 140376063911744 census_dataset.py:167] Parsing /tmp/census_data/adult.data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature keys: ['age', 'workclass', 'fnlwgt', 'education', 'education_num']\n",
      "\n",
      "Age batch   : tf.Tensor([42 57 25 29 17 46 62 37 23 56], shape=(10,), dtype=int32)\n",
      "\n",
      "Label batch : tf.Tensor([False  True  True False False  True False  True False False], shape=(10,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "ds = census_dataset.input_fn(train_file, num_epochs=5, shuffle=True, batch_size=10)\n",
    "\n",
    "for feature_batch, label_batch in ds.take(1):\n",
    "  print('Feature keys:', list(feature_batch.keys())[:5])\n",
    "  print()\n",
    "  print('Age batch   :', feature_batch['age'])\n",
    "  print()\n",
    "  print('Label batch :', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "810fnfY5cYvz"
   },
   "source": [
    "Because `Estimators` expect an `input_fn` that takes no arguments, we typically wrap configurable input function into an object with the expected signature. For this notebook configure the `train_inpf` to iterate over the data twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnQdpEcVcYv0"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "train_inpf = functools.partial(census_dataset.input_fn, train_file, num_epochs=2, shuffle=True, batch_size=64)\n",
    "test_inpf = functools.partial(census_dataset.input_fn, test_file, num_epochs=1, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pboNpNWhcYv4"
   },
   "source": [
    "## Selecting and Engineering Features for the Model\n",
    "\n",
    "Estimators use a system called [feature columns](https://www.tensorflow.org/guide/feature_columns) to describe how the model should interpret each of the raw input features. An Estimator expects a vector of numeric inputs, and feature columns describe how the model should convert each feature.\n",
    "\n",
    "Selecting and crafting the right set of feature columns is key to learning an effective model. A *feature column* can be either one of the raw inputs in the original features `dict` (a *base feature column*), or any new columns created using transformations defined over one or multiple base columns (a *derived feature columns*).\n",
    "\n",
    "A feature column is an abstract concept of any raw or derived variable that can be used to predict the target label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hh-cWdU__Lq"
   },
   "source": [
    "### Base Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKz6LA8_ACI7"
   },
   "source": [
    "#### Numeric columns\n",
    "\n",
    "The simplest `feature_column` is `numeric_column`. This indicates that a feature is a numeric value that should be input to the model directly. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZX0r2T5OcYv6"
   },
   "outputs": [],
   "source": [
    "age = fc.numeric_column('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnLUiaHxcYv-"
   },
   "source": [
    "The model will use the `feature_column` definitions to build the model input. You can inspect the resulting output using the `input_layer` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kREtIPfwcYv_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:41:58.675343 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:41:58.704581 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:41:58.706185 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:41:58.707468 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[42.],\n",
       "       [57.],\n",
       "       [25.],\n",
       "       [29.],\n",
       "       [17.],\n",
       "       [46.],\n",
       "       [62.],\n",
       "       [37.],\n",
       "       [23.],\n",
       "       [56.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(feature_batch, [age]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPuLduCucYwD"
   },
   "source": [
    "The following will train and evaluate a model using only the `age` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9R5eSJ1pcYwE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7570788, 'accuracy_baseline': 0.76377374, 'auc': 0.67835695, 'auc_precision_recall': 0.3113919, 'average_loss': 0.5236284, 'label/mean': 0.23622628, 'loss': 33.432137, 'precision': 0.1572327, 'prediction/mean': 0.25249344, 'recall': 0.00650026, 'global_step': 1018}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=[age])\n",
    "classifier.train(train_inpf)\n",
    "result = classifier.evaluate(test_inpf)\n",
    "\n",
    "clear_output()  # used for display in notebook\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDZGcdTdcYwI"
   },
   "source": [
    "Similarly, we can define a `NumericColumn` for each continuous feature column\n",
    "that we want to use in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqPbUqlxcYwJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.2000e+01, 0.0000e+00, 0.0000e+00, 9.0000e+00, 4.0000e+01],\n",
       "       [5.7000e+01, 1.5024e+04, 0.0000e+00, 1.5000e+01, 3.5000e+01],\n",
       "       [2.5000e+01, 0.0000e+00, 0.0000e+00, 1.3000e+01, 4.0000e+01],\n",
       "       [2.9000e+01, 2.2020e+03, 0.0000e+00, 1.0000e+01, 5.0000e+01],\n",
       "       [1.7000e+01, 0.0000e+00, 0.0000e+00, 7.0000e+00, 3.0000e+01],\n",
       "       [4.6000e+01, 0.0000e+00, 0.0000e+00, 1.1000e+01, 4.0000e+01],\n",
       "       [6.2000e+01, 0.0000e+00, 0.0000e+00, 1.3000e+01, 3.0000e+01],\n",
       "       [3.7000e+01, 0.0000e+00, 0.0000e+00, 1.1000e+01, 4.8000e+01],\n",
       "       [2.3000e+01, 0.0000e+00, 0.0000e+00, 9.0000e+00, 4.8000e+01],\n",
       "       [5.6000e+01, 0.0000e+00, 0.0000e+00, 9.0000e+00, 9.0000e+01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_num = tf.feature_column.numeric_column('education_num')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "my_numeric_columns = [age,education_num, capital_gain, capital_loss, hours_per_week]\n",
    "\n",
    "fc.input_layer(feature_batch, my_numeric_columns).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBGDN97IcYwQ"
   },
   "source": [
    "You could retrain a model on these features by changing the `feature_columns` argument to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XN8k5S95cYwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.78177017\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.75969464\n",
      "auc_precision_recall: 0.5255299\n",
      "average_loss: 1.5091102\n",
      "global_step: 1018\n",
      "label/mean: 0.23622628\n",
      "loss: 96.35225\n",
      "precision: 0.5705344\n",
      "prediction/mean: 0.28281045\n",
      "recall: 0.30811232\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns)\n",
    "classifier.train(train_inpf)\n",
    "\n",
    "result = classifier.evaluate(test_inpf)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "for key,value in sorted(result.items()):\n",
    "  print('%s: %s' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBRq9_AzcYwU"
   },
   "source": [
    "#### Categorical columns\n",
    "\n",
    "To define a feature column for a categorical feature, create a `CategoricalColumn` using one of the `tf.feature_column.categorical_column*` functions.\n",
    "\n",
    "If you know the set of all possible feature values of a column—and there are only a few of them—use `categorical_column_with_vocabulary_list`. Each key in the list is assigned an auto-incremented ID starting from 0. For example, for the `relationship` column we can assign the feature string `Husband` to an integer ID of 0 and \"Not-in-family\" to 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0IjqSi9tcYwV"
   },
   "outputs": [],
   "source": [
    "relationship = fc.categorical_column_with_vocabulary_list(\n",
    "    'relationship',\n",
    "    ['Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RjoWv-7cYwW"
   },
   "source": [
    "This creates a sparse one-hot vector from the raw input feature.\n",
    "\n",
    "The `input_layer` function we're using is designed for DNN models and expects dense inputs. To demonstrate the categorical column we must wrap it in a `tf.feature_column.indicator_column` to create the dense one-hot output (Linear `Estimators` can often skip this dense-step).\n",
    "\n",
    "Note: the other sparse-to-dense option is `tf.feature_column.embedding_column`.\n",
    "\n",
    "Run the input layer, configured with both the `age` and `relationship` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI43CYlncYwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.834218 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.904497 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: VocabularyListCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.905902 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: VocabularyListCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyListCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.906886 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyListCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.909019 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.911054 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:43:13.912049 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6800, shape=(10, 7), dtype=float32, numpy=\n",
       "array([[42.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [57.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [25.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [29.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [17.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [46.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [62.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [37.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [23.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [56.,  1.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(feature_batch, [age, fc.indicator_column(relationship)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTudP7WHcYwb"
   },
   "source": [
    "If we don't know the set of possible values in advance, use the `categorical_column_with_hash_bucket` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pSBaliCcYwb"
   },
   "outputs": [],
   "source": [
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSAPrqQkcYwd"
   },
   "source": [
    "Here, each possible value in the feature column `occupation` is hashed to an integer ID as we encounter them in training. The example batch has a few different occupations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCvQNv36cYwe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transport-moving\n",
      "Sales\n",
      "Exec-managerial\n",
      "Craft-repair\n",
      "?\n",
      "Exec-managerial\n",
      "Farming-fishing\n",
      "Adm-clerical\n",
      "Sales\n",
      "Transport-moving\n"
     ]
    }
   ],
   "source": [
    "for item in feature_batch['occupation'].numpy():\n",
    "    print(item.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP5hN2rAcYwh"
   },
   "source": [
    "If we run `input_layer` with the hashed column, we see that the output shape is `(batch_size, hash_bucket_size)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Y16peWacYwh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:44:25.187920 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:44:25.204374 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:44:25.206940 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupation_result = fc.input_layer(feature_batch, [fc.indicator_column(occupation)])\n",
    "\n",
    "occupation_result.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMW2MzWAcYwk"
   },
   "source": [
    "It's easier to see the actual results if we take the `tf.argmax` over the `hash_bucket_size` dimension. Notice how any duplicate occupations are mapped to the same pseudo-random index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_ryRglmcYwk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([420, 631, 800, 466,  65, 800, 936,  96, 631, 420])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(occupation_result, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1e5NfyKcYwn"
   },
   "source": [
    "Note: Hash collisions are unavoidable, but often have minimal impact on model quality. The effect may be noticable if the hash buckets are being used to compress the input space. See [this notebook](https://colab.research.google.com/github/tensorflow/models/blob/master/samples/outreach/blogs/housing_prices.ipynb) for a more visual example of the effect of these hash collisions.\n",
    "\n",
    "No matter how we choose to define a `SparseColumn`, each feature string is mapped into an integer ID by looking up a fixed mapping or by hashing. Under the hood, the `LinearModel` class is responsible for managing the mapping and creating `tf.Variable` to store the model parameters (model *weights*) for each feature ID. The model parameters are learned through the model training process described later.\n",
    "\n",
    "Let's do the similar trick to define the other categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Z5eUrd_cYwo"
   },
   "outputs": [],
   "source": [
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "\n",
    "my_categorical_columns = [relationship, occupation, education, marital_status, workclass]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASQJM1pEcYwr"
   },
   "source": [
    "It's easy to use both sets of columns to configure a model that uses all these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_i_MLoo9cYws"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82820463\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.8771789\n",
      "auc_precision_recall: 0.66650975\n",
      "average_loss: 0.71846026\n",
      "global_step: 1018\n",
      "label/mean: 0.23622628\n",
      "loss: 45.871574\n",
      "precision: 0.64460987\n",
      "prediction/mean: 0.25261232\n",
      "recall: 0.6079043\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns+my_categorical_columns)\n",
    "classifier.train(train_inpf)\n",
    "result = classifier.evaluate(test_inpf)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "for key,value in sorted(result.items()):\n",
    "  print('%s: %s' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdKEqF6xcYwv"
   },
   "source": [
    "### Derived feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgYaf_48FSU2"
   },
   "source": [
    "#### Make Continuous Features Categorical through Bucketization\n",
    "\n",
    "Sometimes the relationship between a continuous feature and the label is not linear. For example, *age* and *income*—a person's income may grow in the early stage of their career, then the growth may slow at some point, and finally, the income decreases after retirement. In this scenario, using the raw `age` as a real-valued feature column might not be a good choice because the model can only learn one of the three cases:\n",
    "\n",
    "1.  Income always increases at some rate as age grows (positive correlation),\n",
    "2.  Income always decreases at some rate as age grows (negative correlation), or\n",
    "3.  Income stays the same no matter at what age (no correlation).\n",
    "\n",
    "If we want to learn the fine-grained correlation between income and each age group separately, we can leverage *bucketization*. Bucketization is a process of dividing the entire range of a continuous feature into a set of consecutive buckets, and then converting the original numerical feature into a bucket ID (as a categorical feature) depending on which bucket that value falls into. So, we can define a `bucketized_column` over `age` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KT4pjD9AcYww"
   },
   "outputs": [],
   "source": [
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-XOscrEcYwx"
   },
   "source": [
    "`boundaries` is a list of bucket boundaries. In this case, there are 10 boundaries, resulting in 11 age group buckets (from age 17 and below, 18-24, 25-29, ..., to 65 and over).\n",
    "\n",
    "With bucketing, the model sees each bucket as a one-hot feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lr40vm3qcYwy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: BucketizedColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:44:49.809098 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: BucketizedColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: BucketizedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:44:49.810300 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: BucketizedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: BucketizedColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:44:49.812458 140376063911744 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: BucketizedColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[42.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [57.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [25.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [29.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [17.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [46.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [62.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [37.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [23.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [56.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(feature_batch, [age, age_buckets]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_tQI9j8cYw1"
   },
   "source": [
    "#### Learn complex relationships with crossed column\n",
    "\n",
    "Using each base feature column separately may not be enough to explain the data. For example, the correlation between education and the label (earning > 50,000 dollars) may be different for different occupations. Therefore, if we only learn a single model weight for `education=\"Bachelors\"` and `education=\"Masters\"`, we won't capture every education-occupation combination (e.g. distinguishing between `education=\"Bachelors\"` AND `occupation=\"Exec-managerial\"` AND `education=\"Bachelors\" AND occupation=\"Craft-repair\"`).\n",
    "\n",
    "To learn the differences between different feature combinations, we can add *crossed feature columns* to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAPhPzXscYw1"
   },
   "outputs": [],
   "source": [
    "education_x_occupation = tf.feature_column.crossed_column(\n",
    "    ['education', 'occupation'], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeTxMunbcYw5"
   },
   "source": [
    "We can also create a `crossed_column` over more than two columns. Each constituent column can be either a base feature column that is categorical (`SparseColumn`), a bucketized real-valued feature column, or even another `CrossColumn`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8UaBld9cYw7"
   },
   "outputs": [],
   "source": [
    "age_buckets_x_education_x_occupation = tf.feature_column.crossed_column(\n",
    "    [age_buckets, 'education', 'occupation'], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvKmW6U5cYw8"
   },
   "source": [
    "These crossed columns always use hash buckets to avoid the exponential explosion in the number of categories, and put the control over number of model weights in the hands of the user.\n",
    "\n",
    "For a visual example the effect of hash-buckets with crossed columns see [this notebook](https://colab.research.google.com/github/tensorflow/models/blob/master/samples/outreach/blogs/housing_prices.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtjpheB6cYw9"
   },
   "source": [
    "## Define the logistic regression model\n",
    "\n",
    "After processing the input data and defining all the feature columns, we can put them together and build a *logistic regression* model. The previous section showed several types of base and derived feature columns, including:\n",
    "\n",
    "*   `CategoricalColumn`\n",
    "*   `NumericColumn`\n",
    "*   `BucketizedColumn`\n",
    "*   `CrossedColumn`\n",
    "\n",
    "All of these are subclasses of the abstract `FeatureColumn` class and can be added to the `feature_columns` field of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Klmf3OxpcYw-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:45:06.221355 140376063911744 estimator.py:1739] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpw6k2m8r3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fab2841a400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:45:06.304709 140376063911744 estimator.py:201] Using config: {'_model_dir': '/tmp/tmpw6k2m8r3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fab2841a400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "base_columns = [\n",
    "    education, marital_status, relationship, workclass, occupation,\n",
    "    age_buckets,\n",
    "]\n",
    "\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        ['education', 'occupation'], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),\n",
    "]\n",
    "\n",
    "model = tf.estimator.LinearClassifier(\n",
    "    model_dir=tempfile.mkdtemp(),\n",
    "    feature_columns=base_columns + crossed_columns,\n",
    "    optimizer=tf.train.FtrlOptimizer(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRhnPxUucYxC"
   },
   "source": [
    "The model automatically learns a bias term, which controls the prediction made without observing any features. The learned model files are stored in `model_dir`.\n",
    "\n",
    "## Train and evaluate the model\n",
    "\n",
    "After adding all the features to the model, let's train the model. Training a model is just a single command using the `tf.estimator` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlrIBuoecYxD"
   },
   "outputs": [],
   "source": [
    "train_inpf = functools.partial(census_dataset.input_fn, train_file,\n",
    "                               num_epochs=40, shuffle=True, batch_size=64)\n",
    "\n",
    "model.train(train_inpf)\n",
    "\n",
    "clear_output()  # used for notebook display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvY3a9pzcYxH"
   },
   "source": [
    "After the model is trained, evaluate the accuracy of the model by predicting the labels of the holdout data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9nVJEO8cYxI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.84\n",
      "accuracy_baseline: 0.76\n",
      "auc: 0.88\n",
      "auc_precision_recall: 0.69\n",
      "average_loss: 0.35\n",
      "global_step: 20351.00\n",
      "label/mean: 0.24\n",
      "loss: 22.64\n",
      "precision: 0.69\n",
      "prediction/mean: 0.24\n",
      "recall: 0.55\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_inpf)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "for key,value in sorted(results.items()):\n",
    "  print('%s: %0.2f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0fAibNDcYxL"
   },
   "source": [
    "The first line of the output should display something like: `accuracy: 0.84`, which means the accuracy is 84%. You can try using more features and transformations to see if you can do better!\n",
    "\n",
    "After the model is evaluated, we can use it to predict whether an individual has an annual income of over 50,000 dollars given an individual's information input.\n",
    "\n",
    "Let's look in more detail how the model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8R5bz5CxcYxL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income_bracket predicted_class  correct\n",
       "0           <=50K           <=50K     True\n",
       "1           <=50K           <=50K     True\n",
       "2            >50K           <=50K    False\n",
       "3            >50K           <=50K    False\n",
       "4           <=50K           <=50K     True\n",
       "5           <=50K           <=50K     True\n",
       "6           <=50K           <=50K     True\n",
       "7            >50K            >50K     True\n",
       "8           <=50K           <=50K     True\n",
       "9           <=50K           <=50K     True\n",
       "10           >50K           <=50K    False\n",
       "11          <=50K            >50K    False\n",
       "12          <=50K           <=50K     True\n",
       "13          <=50K           <=50K     True\n",
       "14           >50K           <=50K    False\n",
       "15           >50K            >50K     True\n",
       "16          <=50K           <=50K     True\n",
       "17          <=50K           <=50K     True\n",
       "18          <=50K           <=50K     True\n",
       "19           >50K            >50K     True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predict_df = test_df[:20].copy()\n",
    "\n",
    "pred_iter = model.predict(\n",
    "    lambda:easy_input_function(predict_df, label_key='income_bracket',\n",
    "                               num_epochs=1, shuffle=False, batch_size=10))\n",
    "\n",
    "classes = np.array(['<=50K', '>50K'])\n",
    "pred_class_id = []\n",
    "\n",
    "for pred_dict in pred_iter:\n",
    "  pred_class_id.append(pred_dict['class_ids'])\n",
    "\n",
    "predict_df['predicted_class'] = classes[np.array(pred_class_id)]\n",
    "predict_df['correct'] = predict_df['predicted_class'] == predict_df['income_bracket']\n",
    "\n",
    "clear_output()\n",
    "\n",
    "predict_df[['income_bracket','predicted_class', 'correct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_uCpFTicYxN"
   },
   "source": [
    "For a working end-to-end example,  download our [example code](https://github.com/tensorflow/models/tree/master/official/wide_deep/census_main.py) and set the `model_type` flag to `wide`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyKy1lM_3gkL"
   },
   "source": [
    "## Adding Regularization to Prevent Overfitting\n",
    "\n",
    "Regularization is a technique used to avoid overfitting. Overfitting happens when a model performs well on the data it is trained on, but worse on test data that the model has not seen before. Overfitting can occur when a model is excessively complex, such as having too many parameters relative to the number of observed training data. Regularization allows you to control the model's complexity and make the model more generalizable to unseen data.\n",
    "\n",
    "You can add L1 and L2 regularizations to the model with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzMUSBQ03hHx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:28.616790 140376063911744 estimator.py:1739] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpnn36n4gp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 06:48:28.707285 140376063911744 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpnn36n4gp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpnn36n4gp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faaf6574160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:28.708405 140376063911744 estimator.py:201] Using config: {'_model_dir': '/tmp/tmpnn36n4gp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faaf6574160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Parsing /tmp/census_data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:28.727035 140376063911744 census_dataset.py:167] Parsing /tmp/census_data/adult.data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:28.754614 140376063911744 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:29.979316 140376063911744 estimator.py:1113] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:29.980793 140376063911744 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:30.374946 140376063911744 monitored_session.py:222] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:30.457409 140376063911744 session_manager.py:491] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:30.478245 140376063911744 session_manager.py:493] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpnn36n4gp/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:31.070041 140376063911744 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/tmpnn36n4gp/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 44.36142, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:31.543227 140376063911744 basic_session_run_hooks.py:249] loss = 44.36142, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 92.6723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:32.622017 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 92.6723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.961689, step = 101 (1.083 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:32.626173 140376063911744 basic_session_run_hooks.py:247] loss = 24.961689, step = 101 (1.083 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:33.514930 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.310783, step = 201 (0.890 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:33.516530 140376063911744 basic_session_run_hooks.py:247] loss = 21.310783, step = 201 (0.890 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:34.329782 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 27.713894, step = 301 (0.815 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:34.331496 140376063911744 basic_session_run_hooks.py:247] loss = 27.713894, step = 301 (0.815 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:35.223645 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 27.750263, step = 401 (0.894 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:35.225422 140376063911744 basic_session_run_hooks.py:247] loss = 27.750263, step = 401 (0.894 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:36.117098 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 28.841791, step = 501 (0.893 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:36.118752 140376063911744 basic_session_run_hooks.py:247] loss = 28.841791, step = 501 (0.893 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 110.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:37.023103 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 110.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 29.47107, step = 601 (0.906 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:37.025103 140376063911744 basic_session_run_hooks.py:247] loss = 29.47107, step = 601 (0.906 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 123.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:37.832130 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 123.603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.34545, step = 701 (0.809 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:37.833873 140376063911744 basic_session_run_hooks.py:247] loss = 25.34545, step = 701 (0.809 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:38.718172 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 30.35869, step = 801 (0.886 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:38.720234 140376063911744 basic_session_run_hooks.py:247] loss = 30.35869, step = 801 (0.886 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:39.534533 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 22.641308, step = 901 (0.816 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:39.536462 140376063911744 basic_session_run_hooks.py:247] loss = 22.641308, step = 901 (0.816 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:40.420434 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 26.031319, step = 1001 (0.886 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:40.422478 140376063911744 basic_session_run_hooks.py:247] loss = 26.031319, step = 1001 (0.886 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:41.318218 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.85146, step = 1101 (0.898 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:41.320320 140376063911744 basic_session_run_hooks.py:247] loss = 21.85146, step = 1101 (0.898 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:42.131551 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 23.684887, step = 1201 (0.813 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:42.133543 140376063911744 basic_session_run_hooks.py:247] loss = 23.684887, step = 1201 (0.813 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:43.022460 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.597286, step = 1301 (0.891 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:43.024515 140376063911744 basic_session_run_hooks.py:247] loss = 24.597286, step = 1301 (0.891 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 113.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:43.905888 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 113.194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 22.231056, step = 1401 (0.884 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:43.908723 140376063911744 basic_session_run_hooks.py:247] loss = 22.231056, step = 1401 (0.884 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 121.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:44.727085 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 121.775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 22.992348, step = 1501 (0.820 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:44.729196 140376063911744 basic_session_run_hooks.py:247] loss = 22.992348, step = 1501 (0.820 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:45.617621 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 20.596924, step = 1601 (0.890 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:45.619463 140376063911744 basic_session_run_hooks.py:247] loss = 20.596924, step = 1601 (0.890 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:46.434097 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.644773, step = 1701 (0.816 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:46.435900 140376063911744 basic_session_run_hooks.py:247] loss = 21.644773, step = 1701 (0.816 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 113.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:47.319022 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 113.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.840406, step = 1801 (0.885 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:47.320690 140376063911744 basic_session_run_hooks.py:247] loss = 13.840406, step = 1801 (0.885 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:48.213462 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.50061, step = 1901 (0.897 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:48.217335 140376063911744 basic_session_run_hooks.py:247] loss = 24.50061, step = 1901 (0.897 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:49.105127 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 20.557762, step = 2001 (0.892 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:49.109675 140376063911744 basic_session_run_hooks.py:247] loss = 20.557762, step = 2001 (0.892 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 110.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:50.012434 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 110.218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 34.678604, step = 2101 (0.908 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:50.017372 140376063911744 basic_session_run_hooks.py:247] loss = 34.678604, step = 2101 (0.908 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:50.907362 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.675182, step = 2201 (0.892 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:50.909041 140376063911744 basic_session_run_hooks.py:247] loss = 21.675182, step = 2201 (0.892 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:51.720943 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.028896, step = 2301 (0.814 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:51.722601 140376063911744 basic_session_run_hooks.py:247] loss = 19.028896, step = 2301 (0.814 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 113.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:52.605693 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 113.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.790756, step = 2401 (0.885 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:52.607553 140376063911744 basic_session_run_hooks.py:247] loss = 19.790756, step = 2401 (0.885 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 121.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:53.427291 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 121.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 33.803444, step = 2501 (0.822 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:53.429083 140376063911744 basic_session_run_hooks.py:247] loss = 33.803444, step = 2501 (0.822 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:54.317146 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 26.669046, step = 2601 (0.890 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:54.319054 140376063911744 basic_session_run_hooks.py:247] loss = 26.669046, step = 2601 (0.890 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 123.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:55.126284 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 123.588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 22.295452, step = 2701 (0.810 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:55.129545 140376063911744 basic_session_run_hooks.py:247] loss = 22.295452, step = 2701 (0.810 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 123.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:55.936851 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 123.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.710098, step = 2801 (0.875 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:56.004094 140376063911744 basic_session_run_hooks.py:247] loss = 24.710098, step = 2801 (0.875 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 113.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:56.817537 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 113.547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.410133, step = 2901 (0.815 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:56.819258 140376063911744 basic_session_run_hooks.py:247] loss = 21.410133, step = 2901 (0.815 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:57.630714 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.801193, step = 3001 (0.813 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:57.632243 140376063911744 basic_session_run_hooks.py:247] loss = 13.801193, step = 3001 (0.813 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:58.517909 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 34.726273, step = 3101 (0.887 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:58.519638 140376063911744 basic_session_run_hooks.py:247] loss = 34.726273, step = 3101 (0.887 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:59.335230 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.603273, step = 3201 (0.886 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:48:59.405480 140376063911744 basic_session_run_hooks.py:247] loss = 19.603273, step = 3201 (0.886 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:00.222047 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 23.27134, step = 3301 (0.818 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:00.223580 140376063911744 basic_session_run_hooks.py:247] loss = 23.27134, step = 3301 (0.818 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 123.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:01.034891 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 123.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 16.374321, step = 3401 (0.813 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:01.036446 140376063911744 basic_session_run_hooks.py:247] loss = 16.374321, step = 3401 (0.813 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 113.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:01.915213 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 113.595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 23.744987, step = 3501 (0.880 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:01.916905 140376063911744 basic_session_run_hooks.py:247] loss = 23.744987, step = 3501 (0.880 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:02.808903 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.98109, step = 3601 (0.895 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:02.812104 140376063911744 basic_session_run_hooks.py:247] loss = 25.98109, step = 3601 (0.895 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 122.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:03.623573 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 122.747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 22.687468, step = 3701 (0.813 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:03.625528 140376063911744 basic_session_run_hooks.py:247] loss = 22.687468, step = 3701 (0.813 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 111.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:04.518356 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 111.757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.996302, step = 3801 (0.895 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:04.520323 140376063911744 basic_session_run_hooks.py:247] loss = 19.996302, step = 3801 (0.895 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:05.406125 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.203217, step = 3901 (0.888 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:05.408080 140376063911744 basic_session_run_hooks.py:247] loss = 25.203217, step = 3901 (0.888 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 121.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:06.227286 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 121.779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 26.474108, step = 4001 (0.821 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:06.229302 140376063911744 basic_session_run_hooks.py:247] loss = 26.474108, step = 4001 (0.821 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 110.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:07.130127 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 110.764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.324778, step = 4101 (0.903 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:07.132142 140376063911744 basic_session_run_hooks.py:247] loss = 25.324778, step = 4101 (0.903 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:08.022906 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.474874, step = 4201 (0.895 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:08.027449 140376063911744 basic_session_run_hooks.py:247] loss = 25.474874, step = 4201 (0.895 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:08.914071 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 20.918747, step = 4301 (0.891 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:08.918706 140376063911744 basic_session_run_hooks.py:247] loss = 20.918747, step = 4301 (0.891 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 121.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:09.735475 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 121.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.186718, step = 4401 (0.886 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:09.804866 140376063911744 basic_session_run_hooks.py:247] loss = 19.186718, step = 4401 (0.886 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 112.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:10.627295 140376063911744 basic_session_run_hooks.py:680] global_step/sec: 112.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 16.784672, step = 4501 (0.828 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 06:49:10.632505 140376063911744 basic_session_run_hooks.py:247] loss = 16.784672, step = 4501 (0.828 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-bcc822d1f080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         l2_regularization_strength=0.0))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inpf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inpf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1157\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1405\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_l1 = tf.estimator.LinearClassifier(\n",
    "    feature_columns=base_columns + crossed_columns,\n",
    "    optimizer=tf.train.FtrlOptimizer(\n",
    "        learning_rate=0.1,\n",
    "        l1_regularization_strength=10.0,\n",
    "        l2_regularization_strength=0.0))\n",
    "\n",
    "model_l1.train(train_inpf)\n",
    "\n",
    "results = model_l1.evaluate(test_inpf)\n",
    "clear_output()\n",
    "for key in sorted(results):\n",
    "  print('%s: %0.2f' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofmPL212JIy2"
   },
   "outputs": [],
   "source": [
    "model_l2 = tf.estimator.LinearClassifier(\n",
    "    feature_columns=base_columns + crossed_columns,\n",
    "    optimizer=tf.train.FtrlOptimizer(\n",
    "        learning_rate=0.1,\n",
    "        l1_regularization_strength=0.0,\n",
    "        l2_regularization_strength=10.0))\n",
    "\n",
    "model_l2.train(train_inpf)\n",
    "\n",
    "results = model_l2.evaluate(test_inpf)\n",
    "clear_output()\n",
    "for key in sorted(results):\n",
    "  print('%s: %0.2f' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lp1Rfy_k4e7w"
   },
   "source": [
    "These regularized models don't perform much better than the base model. Let's look at the model's weight distributions to better see the effect of the regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb6093N04XlS"
   },
   "outputs": [],
   "source": [
    "def get_flat_weights(model):\n",
    "  weight_names = [\n",
    "      name for name in model.get_variable_names()\n",
    "      if \"linear_model\" in name and \"Ftrl\" not in name]\n",
    "\n",
    "  weight_values = [model.get_variable_value(name) for name in weight_names]\n",
    "\n",
    "  weights_flat = np.concatenate([item.flatten() for item in weight_values], axis=0)\n",
    "\n",
    "  return weights_flat\n",
    "\n",
    "weights_flat = get_flat_weights(model)\n",
    "weights_flat_l1 = get_flat_weights(model_l1)\n",
    "weights_flat_l2 = get_flat_weights(model_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GskJmtfmL0p-"
   },
   "source": [
    "The models have many zero-valued weights caused by unused hash bins (there are many more hash bins than categories in some columns). We can mask these weights when viewing the weight distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rM3agZe3MT3D"
   },
   "outputs": [],
   "source": [
    "weight_mask = weights_flat != 0\n",
    "\n",
    "weights_base = weights_flat[weight_mask]\n",
    "weights_l1 = weights_flat_l1[weight_mask]\n",
    "weights_l2 = weights_flat_l2[weight_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqBpxLLQNEBE"
   },
   "source": [
    "Now plot the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdFK7wWa5_0K"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(weights_base, bins=np.linspace(-3,3,30))\n",
    "plt.title('Base Model')\n",
    "plt.ylim([0,500])\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(weights_l1, bins=np.linspace(-3,3,30))\n",
    "plt.title('L1 - Regularization')\n",
    "plt.ylim([0,500])\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(weights_l2, bins=np.linspace(-3,3,30))\n",
    "plt.title('L2 - Regularization')\n",
    "_=plt.ylim([0,500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mv6knhFa5-iJ"
   },
   "source": [
    "Both types of regularization squeeze the distribution of weights towards zero. L2 regularization has a greater effect in the tails of the distribution eliminating extreme weights. L1 regularization produces more exactly-zero values, in this case it sets ~200 to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MWW1TyjaecRh"
   ],
   "name": "linear.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
